<head>
<title>Simulated Annealing</title>
<link rel="stylesheet" type="text/css" href="style.css">
</head>

<p>You have a problem. You already have some solution to your problem, but you want the very best solution. </p>

<p>There is a set of local modifications that you can make to your solution. After you make a modification, you have a new solution that neighbors your old solution.</p>

<p>You try each modification. If none of the neighboring solutions is better than your solution, then you stay with your solution. If at least one of them is better than your solution, then you change your solution to the best of them. With this new solution now as your solution, you repeat the process. You continue like this until no neighboring solution is better than your solution.</p>

<p>This strategy is an instance of <a href="https://en.wikipedia.org/wiki/Local_search_(optimization)">local search</a>. Specifically, it is <a href="https://en.wikipedia.org/wiki/Hill_climbing">hill climbing</a>.</p>

<p>The issue with this approach is that, although your solution improves as it iteratively changes into its best neighboring solution, the very best solution might never be in the neighborhood of your solution. So, you might never find the very best solution.</p>

<p>Simulated annealing combats this issue. At each iteration, a neighboring solution is drawn at random. Like in hill climbing, if it is better than the current solution, then it becomes the current solution. However, unlike in hill climbing, there is still a probability that it will become the current solution even if it is worse than the current solution. The hope is that by chance the solution will enter a neighborhood where the very best solution lives. </p>

<p>In fact, if you model the sequence of solutions as a <a href="http://mathworld.wolfram.com/MarkovChain.html">Markov chain</a>, then you can show that <a href="http://www.mit.edu/~dbertsim/papers/Optimization/Simulated%20annealing.pdf">simulated annealing converges toward the set of optimal solutions</a>. However, it converges in the limit. Like hill climbing, simulated annealing is a <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)">heuristic</a>. There is no guarantee that it will find a global optimum.</p>

<p>In <a href="http://edoc.sub.uni-hamburg.de/hsu/volltexte/2011/2808/pdf/RR-10-03-01.pdf">the single machine total weighted tardiness problem</a>, you want to process a set of jobs on a single machine. Each job has a processing time, a due date, and a weight that represents its importance. You can only process one job at a time. If a job finishes after its due date, then it is tardy. The importance of tardiness depends on the importance of the job, and you want to minimize the total weighted tardiness for all jobs. </p>

<p>I wrote <a href="https://github.com/jdanray/blog-code/tree/master/sa_smtwtp">a Java program that applies simulated annealing to the single machine total weighted tardiness problem.</a> It is up at <a href="https://github.com/jdanray/blog-code">my GitHub</a>.</p>

<p>Further references:</p>

<p>
http://www.inf.ufpr.br/aurora/disciplinas/topicosia2/livros/search/SA.pdf<br />
http://www.theprojectspot.com/tutorial-post/simulated-annealing-algorithm-for-beginners/6<br/>
http://katrinaeg.com/simulated-annealing.html
</p>
